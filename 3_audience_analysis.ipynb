{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b25bb",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "sw2 = stopwords.words(\"english\")\n",
    "from string import punctuation\n",
    "punctuation = set(punctuation)\n",
    "punctuation.add(\"’\")\n",
    "import datetime\n",
    "import time\n",
    "from random import sample\n",
    "import random\n",
    "from IPython.display import Image\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "# I've put my API keys in a .py file called API_keys.py\n",
    "from my_api_keys import api_key, api_key_secret, access_token, access_token_secret\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel,LdaMulticore, Phrases \n",
    "from gensim.models.phrases import Phraser \n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "#Lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from matplotlib.pyplot import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e2b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Modifications to punctuation and sw lists\n",
    "\n",
    "punctuation = set(punctuation)\n",
    "punctuation.add(\"’\")\n",
    "\n",
    "sw2 = set(sw)\n",
    "addl = (\"|\",\"-\",\"/\",\"•\",\"&\", \"&amp;\")\n",
    "sw2.update(addl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298fc430",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "####  Read daily tweets CSVs into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae473b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "db = pd.DataFrame(columns = ['user_id','screen_name','description','location','friends_count',\n",
    "           'followers_count','totaltweets','date_created', 'tweet_id', 'retweetcount','full_text'])\n",
    "\n",
    "file_location = \"/Users/natebender/Desktop/Repo/footprints/footprints_audience/data/\"\n",
    "files = sorted(os.listdir(file_location))\n",
    "for idx, file in enumerate(files):\n",
    "    \n",
    "    data = \"\".join([file_location,file])\n",
    "    datafile = pd.read_csv(data)\n",
    "\n",
    "    db = db.append(datafile,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce513612",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Check desc stats on overall descriptions before splitting into groups\n",
    "def get_patterns(all_tweets) :\n",
    "\n",
    "    all_desc = all_tweets.dropna()\n",
    "    all_str = \" \".join(all_desc)    \n",
    "    clean = [w for w in all_str.split() if w.lower() not in sw2]\n",
    "    \n",
    "    # Calculate your statistics here\n",
    "    total_tokens = len(clean)\n",
    "    unique_tokens = len(set(clean))\n",
    "    clean_tok_len = [len(w) for w in clean]\n",
    "    avg_token_len = np.mean(clean_tok_len)\n",
    "    lex_diversity = len(set(clean))/len(clean)\n",
    "    top_n = Counter(clean).most_common(20)\n",
    "    \n",
    "    \n",
    "    # Now we'll fill out the dictionary. \n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':unique_tokens,\n",
    "               'avg_token_length':round(avg_token_len,2),\n",
    "               'lexical_diversity':round(lex_diversity,2),\n",
    "               'Top_n':top_n}\n",
    "\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7beb4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_tweets = db.full_text.dropna()\n",
    "print(f'Database: {\"{:,}\".format(len(db.tweet_id))} tweets')\n",
    "print(f'Descriptive stats are:')\n",
    "get_patterns(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05fd21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic Modeling\n",
    "\n",
    "Now we want to dig in deeper and see if we can find groups of tweets that cluster together by distinct theme. \n",
    "\n",
    "We'll accomplish this using LDA (latent dirichlet analysis) modeling. LDA is an unsupervised classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d29ef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for_modeling_unclean = db.full_text.dropna()\n",
    "for_modeling = []\n",
    "\n",
    "for tweet in for_modeling_unclean :\n",
    "    words = [w for w in tweet.split()]# if w not in cop_sw]\n",
    "    words = \" \".join(words)    \n",
    "    for_modeling.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f599f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(for_modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_modeling = random.sample(for_modeling, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf089965",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# database is small enough right now (1/13/22) that taking a sample is not necessary\n",
    "# random.seed(1234)\n",
    "# for_modeling = random.sample(for_modeling, 50000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7343e5a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Updates spaCy's default stop words list with my additional words. \n",
    "# stop_list = ['`',\"Mr.\",\"Mrs.\",\"Ms.\"]\n",
    "# nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a304f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "program_start = time.time()\n",
    "\n",
    "doc_list = []\n",
    "allowed_postags=['NOUN','ADJ','VERB','ADV']\n",
    "\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in for_modeling :\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(doc)\n",
    "    doc_list.append([token.lemma_ for token in pr if token.pos_ in allowed_postags])\n",
    "    \n",
    "program_end = time.time()\n",
    "print('Total time taken to run is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02489329",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "id2word = Dictionary(doc_list)  \n",
    "id2word.filter_extremes(no_below=10, no_above=0.4)  #getting rid of fewer than 10 instances. \n",
    "                                # And no more than words that appear in a certain fraction\n",
    "                                # of the total corpus size (in this case .4)\n",
    "id2word.compactify()  # assign new word ids to all words. \n",
    "corpus = [id2word.doc2bow(word) for word in doc_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861be399",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "\n",
    "program_start = time.time()\n",
    "\n",
    "lda_model = LdaMulticore(corpus=corpus, \n",
    "                             id2word=id2word, \n",
    "                             num_topics=num_topics, \n",
    "                             random_state=1,\n",
    "                             chunksize=30,\n",
    "                             passes=20,\n",
    "                             alpha=0.31,  # sets our priors\n",
    "                             eta=0.91,\n",
    "                             eval_every=1,\n",
    "                             per_word_topics=True,\n",
    "                             workers=1)\n",
    "\n",
    "program_end = time.time()\n",
    "print('Total time taken to run is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d351c13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c20fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6ae7f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb15a96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#pyLDAvis.gensim.prepare(lda_model, corpus, words)\n",
    "#pyLDAvis.gensim_models.prepare(lda_model, corpus,id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64336da",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# create temp lists to pass into dataframe\n",
    "tweetlist = []\n",
    "categorylist = []\n",
    "probabilitylist = []\n",
    "topic_assignments = []\n",
    "\n",
    "\n",
    "# Calculate topic probabilities for each tweet & assign tweets to categories\n",
    "for tweet in for_modeling :\n",
    "    doc = [w for w in tweet.split()]\n",
    "    pr = nlp(\" \".join(doc))\n",
    "    doc = [token.lemma_ for token in pr if token.pos_ in allowed_postags]\n",
    "    doc_new = id2word.doc2bow(doc)\n",
    "\n",
    "    topic_probs = lda_model[doc_new][0]\n",
    "    topic = max(topic_probs,key=lambda x: x[1])\n",
    "    topic_assignments.append(topic[0])    \n",
    "    prob = max(topic_probs)\n",
    "    cat = topic[0]\n",
    "    prob = topic[1]\n",
    "    tweetlist.append(tweet)\n",
    "    categorylist.append(cat)\n",
    "    probabilitylist.append(prob)\n",
    "\n",
    "tweets_df = pd.DataFrame()\n",
    "tweets_df[\"Tweet\"] = tweetlist\n",
    "tweets_df[\"Category\"] = categorylist\n",
    "tweets_df[\"Probability\"] = probabilitylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8729482",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4842e9c",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "newdf = tweets_df.merge(db, left_on='Tweet', right_on='full_text', how='inner')\n",
    "newdf = newdf.drop_duplicates(subset=['tweet_id'], keep='first')\n",
    "newdf[\"date_created\"] = pd.to_datetime(newdf[\"date_created\"], format=\"%Y-%m-%d\") \n",
    "newdf['days'] = newdf['date_created'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51a6cc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# # rename columns from numbers to actual names for final display\n",
    "\n",
    "# newdf['Category'] = newdf['Category'].replace([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "#                                                             ['Random','Activism','Event-focused random','Collaboration',\n",
    "#                                                              'Fossil Fuels', 'Education','CTAs & tracking',\n",
    "#                                                              'End Bad Policy','Aspirational Policy','National FF Policy'])\n",
    "                                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb734c2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "newdf_groupby_category = newdf.groupby('Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed158d79",
   "metadata": {},
   "source": [
    "### Why are we getting so few tweets in the categories if every tweet is being categorized into a category? Across the eight categories we should see the same total as number of tweets in the database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056855a3",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of tweets per category\")\n",
    "print(newdf_groupby_category.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03101b36",
   "metadata": {},
   "source": [
    "### Topic modeling the Environmental group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e7f64",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "newdf_env = newdf[newdf['Category']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ca0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf_env_unique = newdf_env.drop_duplicates(subset = [\"user_id\"])\n",
    "print(len(newdf_env_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_modeling_unclean = newdf_env_unique.full_text.dropna()\n",
    "for_modeling = []\n",
    "\n",
    "for tweet in for_modeling_unclean :\n",
    "    words = [w for w in tweet.split()]# if w not in cop_sw]\n",
    "    words = \" \".join(words)    \n",
    "    for_modeling.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35183347",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_start = time.time()\n",
    "\n",
    "doc_list = []\n",
    "allowed_postags=['NOUN','ADJ','VERB','ADV']\n",
    "\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in for_modeling :\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(doc)\n",
    "    doc_list.append([token.lemma_ for token in pr if token.pos_ in allowed_postags])\n",
    "    \n",
    "program_end = time.time()\n",
    "print('Total time taken to run is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(doc_list)  \n",
    "id2word.filter_extremes(no_below=10, no_above=0.4)  #getting rid of fewer than 10 instances. \n",
    "                                # And no more than words that appear in a certain fraction\n",
    "                                # of the total corpus size (in this case .4)\n",
    "id2word.compactify()  # assign new word ids to all words. \n",
    "corpus = [id2word.doc2bow(word) for word in doc_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "\n",
    "program_start = time.time()\n",
    "\n",
    "lda_model = LdaMulticore(corpus=corpus, \n",
    "                             id2word=id2word, \n",
    "                             num_topics=num_topics, \n",
    "                             random_state=1,\n",
    "                             chunksize=30,\n",
    "                             passes=40,\n",
    "                             alpha=0.31,  # sets our priors\n",
    "                             eta=0.91,\n",
    "                             eval_every=1,\n",
    "                             per_word_topics=True,\n",
    "                             workers=1)\n",
    "\n",
    "program_end = time.time()\n",
    "print('Total time taken to run is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp lists to pass into dataframe\n",
    "tweetlist = []\n",
    "categorylist = []\n",
    "probabilitylist = []\n",
    "topic_assignments = []\n",
    "\n",
    "\n",
    "# Calculate topic probabilities for each tweet & assign tweets to categories\n",
    "for tweet in for_modeling :\n",
    "    doc = [w for w in tweet.split()]\n",
    "    pr = nlp(\" \".join(doc))\n",
    "    doc = [token.lemma_ for token in pr if token.pos_ in allowed_postags]\n",
    "    doc_new = id2word.doc2bow(doc)\n",
    "\n",
    "    topic_probs = lda_model[doc_new][0]\n",
    "    topic = max(topic_probs,key=lambda x: x[1])\n",
    "    topic_assignments.append(topic[0])    \n",
    "    prob = max(topic_probs)\n",
    "    cat = topic[0]\n",
    "    prob = topic[1]\n",
    "    tweetlist.append(tweet)\n",
    "    categorylist.append(cat)\n",
    "    probabilitylist.append(prob)\n",
    "\n",
    "newdf_env_temp = pd.DataFrame()\n",
    "newdf_env_temp[\"Tweet\"] = tweetlist\n",
    "newdf_env_temp[\"Category\"] = categorylist\n",
    "newdf_env_temp[\"Probability\"] = probabilitylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04799c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabac009",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf_env_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf_env_final = newdf_env_temp.merge(tweets_df, left_on='Tweet', right_on='Tweet', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24293f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf_env_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf_envfinal_cats = newdf_env_final.groupby('Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of tweets per category\")\n",
    "print(newdf_envfinal_cats.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf_final = newdf_env_unique[newdf_env_unique['Category']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76963d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e61991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "# pyLDAvis.gensim.prepare(lda_model, corpus, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1491e2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
