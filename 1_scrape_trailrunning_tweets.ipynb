{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808ac66c",
   "metadata": {},
   "source": [
    "### Import packages / setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb69ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "\n",
    "# I've put my API keys in a .py file called API_keys.py\n",
    "from my_api_keys import api_key, api_key_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f4f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the Tweepy API\n",
    "auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61943b",
   "metadata": {},
   "source": [
    "### Function for scraping tweets\n",
    "\n",
    "#### The function scrapes 15k tweets per day and stores in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ac51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapetweets(the_api, search_words, numtweets, numruns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try numruns times\n",
    "\n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['user_id','screen_name','description','location','following_count', \n",
    "                                        'followers_count', 'totaltweets',\n",
    "                                        'date_created', 'tweet_id', 'retweetcount','full_text'])\n",
    "    \n",
    "    program_start = time.time()\n",
    "    for i in range(0, numruns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object, a generator function\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(the_api.search_tweets, \n",
    "                               q=search_words, \n",
    "                               lang=\"en\", \n",
    "                               tweet_mode='extended'\n",
    "                              ).items(numtweets)\n",
    "        \n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        \n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "    \n",
    "        for tweet in tweet_list:\n",
    "            userid = tweet.user.id\n",
    "            username = tweet.user.screen_name\n",
    "            description = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following_count = tweet.user.friends_count\n",
    "            follower_count = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            date_created = tweet.created_at\n",
    "            tweet_id = tweet.id\n",
    "            retweetcount = tweet.retweet_count\n",
    "            full_text = tweet.full_text\n",
    "\n",
    "            # Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [userid, username, description, location, following_count, follower_count, \n",
    "                        totaltweets, date_created, tweet_id, retweetcount, full_text]\n",
    "\n",
    "            # Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "            # increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "                       \n",
    "            \n",
    "            \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "\n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time taken for run {} to complete is {} mins'.format(i+1, duration_run))\n",
    "\n",
    "        time.sleep(0) #15 minute sleep time between runs\n",
    "\n",
    "    # Once all runs have completed, save them to a single csv file:\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.date.today().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_trailrunningtweets.csv'\n",
    "    \n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print(\"\\n\")\n",
    "    print(f'Scraping for {startdate} to {enddate} has completed!')\n",
    "    print('Total time taken to scrape is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219b0dc",
   "metadata": {},
   "source": [
    "### Need to update with pulling followers for each userid in this list\n",
    "\n",
    "Client.get_users_followers\n",
    "https://docs.tweepy.org/en/stable/client.html#tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8181fdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Status' object has no attribute 'get_users_following'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l2/3yz76s950t9fmbgrpxx6nxnc0000gn/T/ipykernel_81805/2500452394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnumruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mscrapetweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/l2/3yz76s950t9fmbgrpxx6nxnc0000gn/T/ipykernel_81805/4136306549.py\u001b[0m in \u001b[0;36mscrapetweets\u001b[0;34m(the_api, search_words, numtweets, numruns)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mfollowing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfriends_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mfollowing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_users_following\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mfollower_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollowers_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mfollowers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_users_followers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Status' object has no attribute 'get_users_following'"
     ]
    }
   ],
   "source": [
    "\n",
    "startdate = \"2022-10-26\"\n",
    "enddate = \"2022-10-31\"\n",
    "\n",
    "search_words = f'#trailrunning OR #running OR #run OR #trail OR #trailrunner OR #trailrun since:{startdate} until:{enddate} -filter:retweets'\n",
    "numtweets=50\n",
    "numruns=1\n",
    "\n",
    "scrapetweets(api, search_words, numtweets, numruns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
