{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808ac66c",
   "metadata": {},
   "source": [
    "### Import packages / setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb69ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "from string import punctuation\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "\n",
    "# I've put my API keys in a .py file called API_keys.py\n",
    "from my_api_keys import api_key, api_key_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f4f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the Tweepy API\n",
    "auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683e2b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Modifications to punctuation and sw lists\n",
    "\n",
    "punctuation = set(punctuation)\n",
    "punctuation.add(\"’\")\n",
    "\n",
    "sw2 = set(sw)\n",
    "addl = (\"|\",\"-\",\"/\",\"•\",\"&\", \"&amp;\")\n",
    "sw2.update(addl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61943b",
   "metadata": {},
   "source": [
    "### Function for scraping tweets\n",
    "\n",
    "#### The function scrapes 15k tweets per day and stores in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ac51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapetweets(the_api, search_words, numtweets, numruns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try numruns times\n",
    "\n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['user_id','screen_name','description','location','friends_count',\n",
    "           'followers_count','totaltweets','date_created', 'tweet_id', 'retweetcount','full_text'])\n",
    "    \n",
    "    program_start = time.time()\n",
    "    for i in range(0, numruns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(the_api.search_tweets, \n",
    "                               q=search_words, \n",
    "                               lang=\"en\", \n",
    "                               tweet_mode='extended'\n",
    "                              ).items(numtweets)\n",
    "        \n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        \n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "    \n",
    "        for tweet in tweet_list:\n",
    "            userid = tweet.user.id\n",
    "            username = tweet.user.screen_name\n",
    "            description = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            date_created = tweet.created_at\n",
    "            tweet_id = tweet.id\n",
    "            retweetcount = tweet.retweet_count\n",
    "            full_text = tweet.full_text\n",
    "\n",
    "            # Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [userid, username, description, location, following, followers, totaltweets,\n",
    "                         date_created, tweet_id, retweetcount, full_text]\n",
    "\n",
    "            # Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "            # increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "                       \n",
    "            \n",
    "            \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "\n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time taken for run {} to complete is {} mins'.format(i+1, duration_run))\n",
    "\n",
    "        time.sleep(920) #15 minute sleep time between runs\n",
    "\n",
    "    # Once all runs have completed, save them to a single csv file:\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.date.today().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_trailrunningtweets.csv'\n",
    "    \n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print(\"\\n\")\n",
    "    print(f'Scraping for {startdate} to {enddate} has completed!')\n",
    "    print('Total time taken to scrape is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8181fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 1 is 2500\n",
      "time taken for run 1 to complete is 1.68 mins\n",
      "no. of tweets scraped for run 2 is 2500\n",
      "time taken for run 2 to complete is 1.66 mins\n",
      "no. of tweets scraped for run 3 is 2500\n",
      "time taken for run 3 to complete is 1.82 mins\n",
      "no. of tweets scraped for run 4 is 2500\n",
      "time taken for run 4 to complete is 1.95 mins\n",
      "no. of tweets scraped for run 5 is 2500\n",
      "time taken for run 5 to complete is 3.28 mins\n",
      "no. of tweets scraped for run 6 is 2500\n",
      "time taken for run 6 to complete is 2.71 mins\n",
      "\n",
      "\n",
      "Scraping for 2022-01-24 to 2022-01-30 has completed!\n",
      "Total time taken to scrape is 105.15 minutes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "startdate = \"2022-01-24\"\n",
    "enddate = \"2022-01-30\"\n",
    "\n",
    "search_words = f'#trailrunning OR #running OR #run OR #trail OR #trailrunner OR #trailrun since:{startdate} until:{enddate} -filter:retweets'\n",
    "numtweets=2500\n",
    "numruns=6\n",
    "\n",
    "scrapetweets(api, search_words, numtweets, numruns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f9074",
   "metadata": {},
   "source": [
    "### Issues to address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9146b5",
   "metadata": {},
   "source": [
    "### Need to ideally also figure out how to scrape not just by the above keywords that search through the tweets text, but also search for \"runner\" in user descriptions. \n",
    "\n",
    "### Like this dude who only tweets about climate change, but identifies as a fell runner in his description. \n",
    "### https://twitter.com/endhunting/with_replies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43765a68",
   "metadata": {},
   "source": [
    "#### 1) Set up code to do this pull weekly\n",
    "#### 2) Set up automatic check to see if user is already in database. Ignore if so. \n",
    "#### 3) Store data in the cloud somewhere instead of locally\n",
    "#### 4) Connect to marketing pipeline somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7f932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
