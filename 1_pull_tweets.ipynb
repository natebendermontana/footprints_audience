{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808ac66c",
   "metadata": {},
   "source": [
    "### Import packages & api setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb69ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9bb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set connection to Tweepy with academic account. I've put my API keys in a .py file called API_keys.py\n",
    "from my_api_keys import bearer_token\n",
    "tweepyclient = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa030d6",
   "metadata": {},
   "source": [
    "### Set custom parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e059772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = \"2022-10-01T00:00:00Z\"\n",
    "end_time = \"2022-11-09T00:00:00Z\"\n",
    "query = '#climate OR #climatechange OR #sustainable OR #sustainability OR #climateaction OR #environment -is:retweet lang:en'\n",
    "response_perpg = 500\n",
    "num_pgs = 20\n",
    "audience_type = \"climate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4e7f7",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4599892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulltweets(query, start_time, end_time, response_perpg, num_pgs):\n",
    "    '''Pulls tweets from Twitter based on a query, limited to a defined timeframe and number of tweets.\n",
    "    Places tweets in a list for further processing'''\n",
    "    \n",
    "    tweet_list = []\n",
    "    func_start = time.time()\n",
    "    \n",
    "    # response is a single \"page\" of tweets. Num of tweets per \"page\" is set by \"max_results\"\n",
    "    for response in tweepy.Paginator(tweepyclient.search_all_tweets, \n",
    "                                     query = query,\n",
    "                                     user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                     tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                     expansions = 'author_id',\n",
    "                                     start_time = start_time,\n",
    "                                     end_time = end_time,\n",
    "                                     max_results=response_perpg):  #pull \"response_perpg\" tweets per response\n",
    "        \n",
    "        # Flag to quit when we need to, the +1 starts the count at 1 rather than 0\n",
    "        if len(tweet_list)+1 > num_pgs :         \n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)  # only 1 request per second allowed\n",
    "            tweet_list.append(response)  # each response is a \"page\" of response_perpg tweets\n",
    "\n",
    "    func_end = time.time()\n",
    "    print(f'Pulled {len(tweet_list)} pages and {len(tweet_list)*response_perpg} tweets')\n",
    "    print('Pull time was {} minutes.'.format(round(func_end - func_start)/60, 2))    \n",
    "    \n",
    "    return(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa7d10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processtweets(tweet_list):\n",
    "    '''Processes tweets into a pandas dataframe'''\n",
    "    \n",
    "    result = []\n",
    "    user_dict = {}\n",
    "    \n",
    "    # Loop through each response object\n",
    "    for response in tweet_list:\n",
    "        # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "        for user in response.includes['users']:\n",
    "            user_dict[user.id] = {'username': user.username, \n",
    "                                  'followers': user.public_metrics['followers_count'],\n",
    "                                  'tweets': user.public_metrics['tweet_count'],\n",
    "                                  'description': user.description,\n",
    "                                  'location': user.location\n",
    "                                 }\n",
    "        for tweet in response.data:\n",
    "            # For each tweet, find the author's information\n",
    "            author_info = user_dict[tweet.author_id]\n",
    "            # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "            result.append({'user_id': tweet.author_id, \n",
    "                           'username': author_info['username'],\n",
    "                           'follower_count': author_info['followers'],\n",
    "                           'total_tweets': author_info['tweets'],\n",
    "                           'description': author_info['description'],\n",
    "                           'location': author_info['location'],\n",
    "                           'tweet_id' : tweet.id,\n",
    "                           'text': tweet.text,\n",
    "                           'created_at': tweet.created_at,\n",
    "                           'retweets_count': tweet.public_metrics['retweet_count'],\n",
    "                           'replies_count': tweet.public_metrics['reply_count'],\n",
    "                           'likes_count': tweet.public_metrics['like_count'],\n",
    "                           'quote_count': tweet.public_metrics['quote_count']\n",
    "                          })\n",
    "\n",
    "    # Change this list of dictionaries into a dataframe\n",
    "    df = pd.DataFrame(result)\n",
    "    \n",
    "    # Let's see how long it took to grab all tweets and how many were pulled\n",
    "    print('Processed {} tweets'.format(len(df)))\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b83c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullandprocess(query, start_time, end_time, response_perpg, num_pgs, audience_type):\n",
    "    '''Combines the pull and the process functions into a single function that pulls, processes, and writes\n",
    "    dataframe to a csv file stored locally'''\n",
    "    \n",
    "    tweet_list = pulltweets(query, start_time, end_time, response_perpg, num_pgs)\n",
    "    \n",
    "    tweetsdf = processtweets(tweet_list)\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.date.today().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Define working path\n",
    "    path = os.getcwd()\n",
    "    \n",
    "    # define filename location, timestamp, and custom audience type\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_tweets_' + audience_type + '.csv'\n",
    "    \n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    tweetsdf.to_csv(filename, index = False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c36c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 20 pages and 10000 tweets\n",
      "Pull time was 0.7666666666666667 minutes.\n",
      "Processed 9983 tweets\n"
     ]
    }
   ],
   "source": [
    "pullandprocess(query, start_time, end_time, response_perpg, num_pgs, audience_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
