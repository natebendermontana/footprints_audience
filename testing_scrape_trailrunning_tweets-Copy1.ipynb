{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808ac66c",
   "metadata": {},
   "source": [
    "### Import packages / setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb69ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9bb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've put my API keys in a .py file called API_keys.py\n",
    "from my_api_keys import bearer_token\n",
    "tweepyclient = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the Tweepy API\n",
    "#auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "#auth.set_access_token(access_token, access_token_secret)\n",
    "#api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACADEMIC ACCOUNT\n",
    "# Set connection to Tweepy\n",
    "\n",
    "# From the \"project set-up\" experience\n",
    "#api_key = 'e9hKR9ngqwXalms6q4IndwoAz'\n",
    "#api_key_secret = 'mQzPQWyQvO3ikOqMGjJfKpbFTMcSJKNE9vvywpuSi1Jv0HAjGt'\n",
    "#bearer_token = 'AAAAAAAAAAAAAAAAAAAAADMxgwEAAAAAcEBEj8u5Qy'  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrapetweetstest(query, start_time, end_time, num_tweets):\n",
    "    \n",
    "    tweet_list = []\n",
    "    \n",
    "    for response in tweepy.Paginator(tweepyclient.search_all_tweets, \n",
    "                                     query = query,\n",
    "                                     user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                     tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                     expansions = 'author_id',\n",
    "                                     start_time = start_time,\n",
    "                                     end_time = end_time,\n",
    "                                  max_results=10):\n",
    "        tweet_list.append(response)\n",
    "        print(len(tweet_list))\n",
    "        \n",
    "        # Begin scraping the tweets individually:\n",
    "        #noTweets = 0\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ae422",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"2022-10-31T00:00:00Z\"\n",
    "end_time = \"2022-11-01T00:00:00Z\"\n",
    "query = f'#trailrunning OR #running OR #run OR #trail OR #trailrunner OR #trailrun -is:retweet lang:en'\n",
    "num_tweets=100\n",
    "\n",
    "scrapetweetstest(query, start_time, end_time, num_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69dae37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ff0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc1660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64997861",
   "metadata": {},
   "source": [
    "### start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d3f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301d997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b353b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d090c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "user_dict = {}\n",
    "# Loop through each response object\n",
    "for response in trail_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    for tweet in response.data:\n",
    "        # For each tweet, find the author's information\n",
    "        author_info = user_dict[tweet.author_id]\n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        result.append({'user_id': tweet.author_id, \n",
    "                       'username': author_info['username'],\n",
    "                       'follower_count': author_info['followers'],\n",
    "                       'total_tweets': author_info['tweets'],\n",
    "                       'description': author_info['description'],\n",
    "                       'location': author_info['location'],\n",
    "                       'tweet_id' : tweet.id,\n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'retweets_count': tweet.public_metrics['retweet_count'],\n",
    "                       'replies_count': tweet.public_metrics['reply_count'],\n",
    "                       'likes_count': tweet.public_metrics['like_count'],\n",
    "                       'quote_count': tweet.public_metrics['quote_count']\n",
    "                      })\n",
    "\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c449ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61943b",
   "metadata": {},
   "source": [
    "### Function for scraping tweets\n",
    "\n",
    "#### The function scrapes 15k tweets per day and stores in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapetweets(search_words, start_time, end_time, numtweets, numruns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try numruns times\n",
    "\n",
    "    # Define a pandas dataframe to store the data:\n",
    "    db_tweets = pd.DataFrame(columns = ['user_id','screen_name','description','location','following_count', \n",
    "                                        'followers_count', 'totaltweets',\n",
    "                                        'date_created', 'tweet_id', 'retweetcount','full_text'])\n",
    "    \n",
    "    program_start = time.time()\n",
    "    for i in range(0, numruns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object, a generator function\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepyclient.search_all_tweets(query=search_words,\n",
    "                                                end_time=end_time,\n",
    "                                                start_time=start_time\n",
    "                              )\n",
    "        \n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        \n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "    \n",
    "        for tweet in tweet_list:\n",
    "            userid = tweet.user.id\n",
    "            username = tweet.user.screen_name\n",
    "            description = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following_count = tweet.user.friends_count\n",
    "            follower_count = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            date_created = tweet.created_at\n",
    "            tweet_id = tweet.id\n",
    "            retweetcount = tweet.retweet_count\n",
    "            full_text = tweet.full_text\n",
    "\n",
    "            # Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [userid, username, description, location, following_count, follower_count, \n",
    "                        totaltweets, date_created, tweet_id, retweetcount, full_text]\n",
    "\n",
    "            # Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "            # increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "                          \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "\n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time taken for run {} to complete is {} mins'.format(i+1, duration_run))\n",
    "\n",
    "        time.sleep(0) #15 minute sleep time between runs\n",
    "        \n",
    "    # Once all runs have completed, save them to a single csv file:\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.date.today().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_trailrunningtweets.csv'\n",
    "    \n",
    "    # Store dataframe in csv with creation date timestamp\n",
    "    #db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print(\"\\n\")\n",
    "    print(f'Scraping for {startdate} to {enddate} has completed!')\n",
    "    print('Total time taken to scrape is {} minutes.'.format(round(program_end - program_start)/60, 2))\n",
    "    \n",
    "    return db_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219b0dc",
   "metadata": {},
   "source": [
    "### Need to update with pulling followers for each userid in this list\n",
    "\n",
    "Client.get_users_followers\n",
    "https://docs.tweepy.org/en/stable/client.html#tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8181fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set time, search parameters and run scrapetweets function\n",
    "\n",
    "start_time = \"2022-10-27T12:00:00Z\"\n",
    "end_time = \"2022-11-01T12:00:00Z\"\n",
    "\n",
    "search_words = f'#trailrunning OR #running OR #run OR #trail OR #trailrunner OR #trailrun -is:retweet'\n",
    "numtweets=100\n",
    "numruns=1\n",
    "\n",
    "scrapetweets(search_words, start_time, end_time, numtweets, numruns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61da8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80179b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186eb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72755a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a770d1c",
   "metadata": {},
   "source": [
    "### testing adding followers & following to this intermediate dataframe db_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a94e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tweets = pd.read_csv(\"data/20221102_000000_trailrunningtweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tweets_unique = db_tweets.drop_duplicates(subset = [\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ca7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followers_id(person):\n",
    "    followersid = []\n",
    "    count=0\n",
    "    user=api.get_user(screen_name=person)\n",
    "    user_id=user.id\n",
    "    number_of_followers=user.followers_count\n",
    "    status = tweepy.Cursor(api.get_follower_ids, screen_name=person, tweet_mode=\"extended\").items()\n",
    "    for i in range(0,number_of_followers):\n",
    "        follower=next(status)\n",
    "        followersid.append(follower)\n",
    "        count += 1\n",
    "    return followersid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_followers_id(\"37chandler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afceb73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"2022-10-20T00:00:00Z\"\n",
    "end_time = \"2022-11-01T00:00:00Z\"\n",
    "query = f'#trailrunning OR #running OR #run OR #trail OR #trailrunner OR #trailrun -is:retweet'\n",
    "\n",
    "tweet_list = []\n",
    "\n",
    "tweets = tweepyclient.search_all_tweets(query=query,\n",
    "                                   end_time=end_time,\n",
    "                                   start_time=start_time,\n",
    "                                   max_results=500)\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ef1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
